MY_CATEGORY = "ğŸ‘ MieNodes/ğŸ‘ Audio Operator"

try:
    import torch
except ImportError:
    # ç¡®ä¿åœ¨ ComfyUI ç¯å¢ƒä¸­ torch å¯ç”¨
    raise ImportError("PyTorch (torch) is required for audio processing nodes but was not found.")


class WavConcat(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"force_output": True}),
            },
            "optional": {
                "audio2": ("AUDIO", {"force_output": True}),
                "mute_audio1": ("BOOLEAN", {"default": False, "label": "Mute Audio 1"}),
                "mute_audio2": ("BOOLEAN", {"default": False, "label": "Mute Audio 2"}),
                # æ–°å¢ï¼šå¼€å§‹é—´éš” (start)
                "start_spacer": ("FLOAT",
                                 {"default": 0.0, "min": 0.0, "max": 5.0, "step": 0.1, "label": "Start Silence (s)"}),
                # æ–°å¢ï¼šä¸­é—´é—´éš” (middle)
                "middle_spacer": ("FLOAT",
                                  {"default": 0.0, "min": 0.0, "max": 5.0, "step": 0.1, "label": "Middle Silence (s)"}),
                # æ–°å¢ï¼šç»“æŸé—´éš” (end)
                "end_spacer": ("FLOAT",
                               {"default": 0.0, "min": 0.0, "max": 5.0, "step": 0.1, "label": "End Silence (s)"}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("concatenated_audio",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def _create_silence_tensor(self, duration, sample_rate, reference_tensor):
        """æ ¹æ®æ—¶é•¿å’Œå‚è€ƒå¼ é‡åˆ›å»ºé™éŸ³å¼ é‡"""
        if duration <= 0.0:
            return None

        num_silent_samples = int(duration * sample_rate)

        # ä¿æŒ Batch å’Œ Channel ç»´åº¦ä¸å˜ï¼Œæ—¶é—´ç»´åº¦è®¾ä¸ºé™éŸ³é•¿åº¦
        silent_shape = list(reference_tensor.shape)
        silent_shape[-1] = num_silent_samples

        # åˆ›å»ºå…¨é›¶é™éŸ³å¼ é‡ï¼Œä¿æŒ dtype å’Œ device ä¸€è‡´
        silence_tensor = torch.zeros(tuple(silent_shape),
                                     dtype=reference_tensor.dtype,
                                     device=reference_tensor.device)
        return silence_tensor

    def execute(self, audio1, audio2=None, mute_audio1=False, mute_audio2=False, start_spacer=0.0, middle_spacer=0.0,
                end_spacer=0.0):

        # 1. æå– audio1 æ•°æ®
        sample_rate = audio1["sample_rate"]
        waveform1 = audio1["waveform"]

        # å¦‚æœ mute_audio1 ä¸º Trueï¼Œåˆ™å°† waveform1 æ›¿æ¢ä¸ºé™éŸ³
        if mute_audio1:
            duration1 = waveform1.shape[-1] / sample_rate
            waveform1 = self._create_silence_tensor(duration1, sample_rate, waveform1)

        # 2. åˆå§‹åŒ–æ‹¼æ¥åˆ—è¡¨
        parts_to_concat = []

        # a. å¼€å§‹é—´éš”
        start_silence = self._create_silence_tensor(start_spacer, sample_rate, waveform1)
        if start_silence is not None:
            parts_to_concat.append(start_silence)

        # b. Audio 1
        if waveform1 is not None:
            parts_to_concat.append(waveform1)

        # 3. å¤„ç†å¯é€‰çš„ audio2
        if audio2 is not None:
            # éªŒè¯é‡‡æ ·ç‡
            if sample_rate != audio2["sample_rate"]:
                raise ValueError(
                    f"Audio sample rate mismatch: audio1 has {sample_rate} Hz, but audio2 has {audio2['sample_rate']} Hz.")

            waveform2 = audio2["waveform"]

            # å¦‚æœ mute_audio2 ä¸º Trueï¼Œåˆ™å°† waveform2 æ›¿æ¢ä¸ºé™éŸ³
            if mute_audio2:
                duration2 = waveform2.shape[-1] / sample_rate
                waveform2 = self._create_silence_tensor(duration2, sample_rate, waveform2)

            # c. ä¸­é—´é—´éš”
            middle_silence = self._create_silence_tensor(middle_spacer, sample_rate, waveform1)
            if middle_silence is not None:
                parts_to_concat.append(middle_silence)

            # d. Audio 2
            if waveform2 is not None:
                parts_to_concat.append(waveform2)

        # e. ç»“æŸé—´éš”
        end_silence = self._create_silence_tensor(end_spacer, sample_rate, waveform1)
        if end_silence is not None:
            parts_to_concat.append(end_silence)

        # 4. æ‰§è¡Œæ‹¼æ¥
        if not parts_to_concat:
            # å¦‚æœæ‰€æœ‰è¾“å…¥éƒ½æ— æ•ˆï¼Œåˆ™è¿”å›ä¸€ä¸ªç©ºçš„éŸ³é¢‘
            return ({"waveform": torch.empty(1, 2, 0), "sample_rate": sample_rate},)

        # æ²¿ç€æœ€åä¸€ä¸ªç»´åº¦ï¼ˆæ—¶é—´ç»´åº¦ï¼‰æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†
        final_waveform = torch.cat(parts_to_concat, dim=-1)

        # 5. è¿”å›æ–°çš„ AUDIO ç»“æ„
        result_audio = {
            "waveform": final_waveform,
            "sample_rate": sample_rate
        }

        return (result_audio,)