import requests
import time

from .utils import mie_log

MY_CATEGORY = "ğŸ‘ MieNodes/ğŸ‘ LLM Service Config"


# å¼•å…¥ time æ¨¡å—ç”¨äºåœ¨é‡è¯•é—´å¢åŠ å»¶è¿Ÿ
class GeneralLLMServiceConnector:
    def __init__(self, api_url, api_token, model, timeout=30, max_retries=3, retry_delay=5):
        self.api_url = api_url
        self.api_token = api_token
        self.model = model
        self.timeout = timeout
        self.max_retries = max_retries  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_delay = retry_delay  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰

    def generate_payload(self, messages, **kwargs):
        """
        ç”Ÿæˆæ ‡å‡†çš„ OpenAI å…¼å®¹æœåŠ¡çš„ Payloadã€‚
        å­ç±»å¦‚æœéœ€è¦ä¸åŒçš„é»˜è®¤å‚æ•°æˆ–ç»“æ„ï¼Œå¯ä»¥é‡å†™æ­¤æ–¹æ³•ã€‚
        """
        return {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "response_format": {"type": "text"},
        }

    def invoke(self, messages, **kwargs):
        """
        è°ƒç”¨ LLM æœåŠ¡ï¼Œå¹¶å®ç°é’ˆå¯¹ç¬æ—¶é”™è¯¯çš„é‡è¯•æœºåˆ¶ã€‚
        é‡è¯•åŒ…æ‹¬ï¼šTimeout, ConnectionError, å’Œ 5xx çŠ¶æ€ç ã€‚
        """
        payload = self.generate_payload(messages, **kwargs)

        headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json"
        }

        # å¢åŠ é‡è¯•å¾ªç¯
        for attempt in range(self.max_retries):
            is_last_attempt = (attempt == self.max_retries - 1)

            try:
                response = requests.post(self.api_url, json=payload, headers=headers, timeout=self.timeout)

                # è¯·æ±‚æˆåŠŸï¼ˆçŠ¶æ€ç  200ï¼‰
                if response.status_code == 200:
                    response_data = response.json()
                    try:
                        return response_data["choices"][0]["message"]["content"]
                    except KeyError:
                        raise ValueError(
                            f"Unexpected response format: missing 'content'. Response: {response.text[:200]}...")

                # æœåŠ¡å™¨é”™è¯¯ï¼ˆ5xxï¼‰: ç¬æ—¶é”™è¯¯ï¼Œå°è¯•é‡è¯•
                elif 500 <= response.status_code < 600:
                    error_message = f"API returned {response.status_code}. Server Side Error."
                    if is_last_attempt:
                        raise Exception(
                            f"{error_message} Max retries ({self.max_retries}) exceeded. Response: {response.text}")
                    else:
                        mie_log(
                            f"{error_message} Retrying in {self.retry_delay} seconds... (Attempt {attempt + 1}/{self.max_retries})")
                        time.sleep(self.retry_delay)
                        continue  # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯é‡è¯•

                # å…¶ä»–é 200/5xx é”™è¯¯ï¼ˆä¾‹å¦‚ 4xx å®¢æˆ·ç«¯é”™è¯¯ï¼‰: ä¸é‡è¯•ï¼Œç›´æ¥æŠ›å‡º
                else:
                    raise Exception(f"Request failed with status code {response.status_code}: {response.text}")

            # å¤„ç† Timeout å’Œ ConnectionError ç­‰ç¬æ—¶è¯·æ±‚å¼‚å¸¸
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                error_type = type(e).__name__
                if is_last_attempt:
                    raise Exception(f"Request failed: {error_type}. Max retries ({self.max_retries}) exceeded.")
                else:
                    mie_log(
                        f"Request failed with {error_type}. Retrying in {self.retry_delay} seconds... (Attempt {attempt + 1}/{self.max_retries})")
                    time.sleep(self.retry_delay)
                    continue  # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯é‡è¯•

            # å…¶ä»–éç¬æ—¶ requests é”™è¯¯ (ä¾‹å¦‚ SSL/Proxy é”™è¯¯)
            except requests.exceptions.RequestException as e:
                # è¿™ç±»é”™è¯¯é€šå¸¸ä¸æ˜¯ç¬æ—¶çš„ï¼Œç›´æ¥æŠ›å‡º
                raise Exception(f"A non-retryable request error occurred: {e}")

        # ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼Œä½†ä»¥é˜²ä¸‡ä¸€
        raise Exception(f"LLM Service failed after {self.max_retries} attempts due to an unknown error.")

    def get_state(self):
        """è¿”å›ç”¨äºæ¯”è¾ƒçŠ¶æ€çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        # æ¢å¤ä¸ºåŸå…ˆçš„æ— åˆ†éš”ç¬¦å½¢å¼ï¼Œä¿è¯ä¸å†å²è¡Œä¸ºä¸€è‡´ï¼ˆé¿å…å›å½’ï¼‰
        return f"{self.api_url}{self.api_token}{self.model}"


class StandardOpenAICompatibleConnector(GeneralLLMServiceConnector):
    """
    é’ˆå¯¹ SiliconFlow, ZhiPu, Kimi ç­‰å…·æœ‰æ ‡å‡† OpenAI å…¼å®¹å‚æ•°çš„ APIã€‚
    """

    def generate_payload(self, messages, **kwargs):
        # å°è£… OpenAI å…¼å®¹æœåŠ¡çš„é€šç”¨å‚æ•°
        return {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "max_tokens": kwargs.get("max_tokens", 512),
            "temperature": kwargs.get("temperature", 0.7),
            "top_p": kwargs.get("top_p", 0.9),
            "top_k": kwargs.get("top_k", 50),
            "frequency_penalty": kwargs.get("frequency_penalty", 0.5),
            "n": kwargs.get("n", 1),
            "response_format": {"type": "text"},
        }


# é€‚é…SiliconFlow
class SiliconFlowConnectorGeneral(StandardOpenAICompatibleConnector):
    api_url = "https://api.siliconflow.cn/v1/chat/completions"

    def __init__(self, api_token, model):
        super().__init__(self.api_url, api_token, model)


class ZhiPuConnectorGeneral(StandardOpenAICompatibleConnector):
    api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

    def __init__(self, api_token, model):
        super().__init__(self.api_url, api_token, model)


class KimiConnectorGeneral(StandardOpenAICompatibleConnector):
    api_url = "https://api.moonshot.cn/v1/chat/completions"

    def __init__(self, api_token, model):
        super().__init__(self.api_url, api_token, model)


class GithubModelsConnectorGeneral(GeneralLLMServiceConnector):
    api_url = "https://models.github.ai/inference/chat/completions"

    def __init__(self, api_token, model):
        # ç»§æ‰¿ GeneralLLMServiceConnector çš„é»˜è®¤ Payload
        super().__init__(self.api_url, api_token, model)


class BailianLLMServiceConnector(GeneralLLMServiceConnector):
    api_url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

    def __init__(self, api_token, model):
        super().__init__(self.api_url, api_token, model)

    def generate_payload(self, messages, **kwargs):
        # é˜¿é‡Œç™¾ç‚¼ï¼ˆé€šä¹‰åƒé—®ï¼‰çš„ Payload å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œè¿™é‡Œä¿ç•™å…¶ç‰¹æ®Šæ€§
        return {
            "model": self.model,
            "messages": messages,
            "stream": False,
            # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–å‚æ•°
        }


class DeepSeekConnectorGeneral(GeneralLLMServiceConnector):
    api_url = "https://api.deepseek.com/chat/completions"

    def __init__(self, api_token, model):
        super().__init__(self.api_url, api_token, model)


class GeminiConnectorGeneral(GeneralLLMServiceConnector):
    base_url = "https://generativelanguage.googleapis.com/v1beta/models"

    def __init__(self, api_token, model):
        self.model = model
        self.api_token = api_token  # éœ€è¦ä¿ç•™ï¼Œå› ä¸º Token æ˜¯é€šè¿‡ URL å‚æ•°ä¼ é€’çš„
        api_url = f"{self.base_url}/{model}:generateContent"
        # ç»§æ‰¿åŸºç±»çš„ timeout, max_retries, retry_delay
        super().__init__(api_url, api_token, model)

    def generate_payload(self, messages, **kwargs):
        # é€‚é… Gemini ç‰¹æœ‰çš„ Payload æ ¼å¼
        contents = []
        for msg in messages:
            # Gemini è¦æ±‚ 'role' æ˜¯ 'user' æˆ– 'model'
            role = "user" if msg["role"] == "user" else "model"
            contents.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })
        return {
            "contents": contents,
            "generationConfig": {
                "maxOutputTokens": kwargs.get("max_tokens", 10240),
                "temperature": kwargs.get("temperature", 0.7),
                "topP": kwargs.get("top_p", 0.9),
                "topK": kwargs.get("top_k", 50)
            }
        }

    def invoke(self, messages, **kwargs):
        """
        é‡å†™ invoke æ–¹æ³•ä»¥å¤„ç† Gemini ç‰¹æœ‰çš„è®¤è¯æ–¹å¼ (URL å‚æ•°) å’Œå“åº”è§£æï¼Œ
        ä½†é‡è¯•å¾ªç¯é€»è¾‘å°†**å°½å¯èƒ½ä¾èµ–åŸºç±»**ã€‚

        ä¸ºäº†åˆ©ç”¨åŸºç±»çš„é‡è¯•å¾ªç¯ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªè¾…åŠ©æ–¹æ³•æˆ–è€…åœ¨ invoke ä¸­æ¨¡æ‹ŸåŸºç±»é€»è¾‘
        æˆ–è€…**ç›´æ¥åœ¨ invoke ä¸­å®ç°è‡ªå·±çš„å¾ªç¯å¹¶ä¸“æ³¨äº Geminiçš„ç‰¹æ®Šæ€§**ã€‚
        ç”±äºè®¤è¯æ–¹å¼ï¼ˆURLå‚æ•°ï¼‰å’Œè¿”å›è§£æï¼ˆcandidates åˆ—è¡¨ï¼‰ä¸åŒï¼Œ
        æœ€æ¸…æ™°çš„åŠæ³•æ˜¯é‡å†™æ•´ä¸ª invokeï¼Œä½†åˆ©ç”¨åŸºç±»çš„é‡è¯•å‚æ•°ã€‚
        """
        payload = self.generate_payload(messages, **kwargs)
        headers = {"Content-Type": "application/json"}
        # Gemini è®¤è¯æ–¹å¼ï¼šToken ä½œä¸º URL å‚æ•°
        url = f"{self.api_url}?key={self.api_token}"

        for attempt in range(self.max_retries):
            is_last_attempt = (attempt == self.max_retries - 1)

            try:
                response = requests.post(url, json=payload, headers=headers, timeout=self.timeout)

                if response.status_code == 200:
                    response_data = response.json()
                    # é€‚é… Gemini å“åº”è§£æ: candidates -> content -> parts -> text
                    if not response_data.get("candidates"):
                        raise ValueError(f"No candidates in response. Response: {response.text}")
                    return response_data["candidates"][0]["content"]["parts"][0]["text"]

                # 5xx ç¬æ—¶é”™è¯¯å¤„ç† (åˆ©ç”¨åŸºç±»é€»è¾‘)
                elif 500 <= response.status_code < 600:
                    error_message = f"API returned {response.status_code}. Server Side Error."
                    if is_last_attempt:
                        raise Exception(
                            f"{error_message} Max retries ({self.max_retries}) exceeded. Response: {response.text}")
                    else:
                        mie_log(
                            f"[{self.model}] {error_message} Retrying in {self.retry_delay} seconds... (Attempt {attempt + 1}/{self.max_retries})")
                        time.sleep(self.retry_delay)
                        continue  # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯é‡è¯•

                else:
                    raise Exception(f"Request failed with status code {response.status_code}: {response.text}")

            # å¤„ç† Timeout å’Œ ConnectionError ç­‰ç¬æ—¶è¯·æ±‚å¼‚å¸¸ (åˆ©ç”¨åŸºç±»é€»è¾‘)
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                error_type = type(e).__name__
                if is_last_attempt:
                    raise Exception(f"Request failed: {error_type}. Max retries ({self.max_retries}) exceeded.")
                else:
                    mie_log(
                        f"[{self.model}] Request failed with {error_type}. Retrying in {self.retry_delay} seconds... (Attempt {attempt + 1}/{self.max_retries})")
                    time.sleep(self.retry_delay)
                    continue  # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯é‡è¯•

            # å…¶ä»– requests é”™è¯¯
            except requests.exceptions.RequestException as e:
                raise Exception(f"[{self.model}] A non-retryable request error occurred: {e}")

            # å…¶ä»– Python å¼‚å¸¸ (ä¾‹å¦‚ JSON è§£æé”™è¯¯)
            except Exception as e:
                # æ•è·å…¶ä»–éç½‘ç»œé”™è¯¯ï¼Œä¾‹å¦‚ ValueErrorï¼ˆå¦‚ No candidates in responseï¼‰
                raise Exception(f"[{self.model}] Unknown error during API call: {e}")

        # ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
        raise Exception(f"[{self.model}] LLM Service failed after {self.max_retries} attempts due to an unknown error.")


class SetGeneralLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://api.siliconflow.cn/v1/chat/completions"}),
                "api_token": ("STRING", {"default": "token"}),
                "model_select": ("STRING", {"default": "deepseek-ai/DeepSeek-V3"}),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_url, api_token, model_select):
        return GeneralLLMServiceConnector(api_url, api_token, model_select),


class SetGithubModelsLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "openai/gpt-4.1",
                        "Custom",
                    ],
                    {"default": "openai/gpt-4.1"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "Enter custom model name (used when model_select is 'Custom')",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "openai/gpt-4.1"  # é»˜è®¤æ¨¡å‹
        return GithubModelsConnectorGeneral(api_token, model),


class SetSiliconFlowLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "deepseek-ai/DeepSeek-V3",
                        "deepseek-ai/DeepSeek-V3.1-Terminus",
                        "deepseek-ai/DeepSeek-V3.2-Exp",
                        "THUDM/GLM-Z1-9B-0414",
                        "THUDM/GLM-4-32B-0414",
                        "Qwen/Qwen3-8B",
                        "moonshotai/Kimi-K2-Instruct",
                        "Custom",
                    ],
                    {"default": "THUDM/GLM-4-32B-0414"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "Enter custom model name (used when model_select is 'Custom')",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "THUDM/GLM-4-32B-0414"  # é»˜è®¤æ¨¡å‹
        return SiliconFlowConnectorGeneral(api_token, model),


class SetZhiPuLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "GLM-4-Flash-250414",
                        "Custom",
                    ],
                    {"default": "GLM-4-Flash-250414"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "Enter custom model name (used when model_select is 'Custom')",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "GLM-4-Flash-250414"  # é»˜è®¤æ¨¡å‹
        return ZhiPuConnectorGeneral(api_token, model),


class SetKimiLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "kimi-k2-0711-preview",
                        "moonshot-v1-8k",
                        "moonshot-v1-32k",
                        "moonshot-v1-128k",
                        "moonshot-v1-auto",
                        "kimi-latest",
                        "moonshot-v1-8k-vision-preview",
                        "moonshot-v1-32k-vision-preview",
                        "moonshot-v1-128k-vision-preview",
                        "kimi-thinking-preview",
                        "Custom",
                    ],
                    {"default": "kimi-k2-0711-preview"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "Enter custom model name (used when model_select is 'Custom')",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "kimi-k2-0711-preview"  # é»˜è®¤æ¨¡å‹
        return KimiConnectorGeneral(api_token, model),


class SetDeepSeekLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "deepseek-chat",
                        "deepseek-reasoner",
                        "Custom",
                    ],
                    {"default": "deepseek-chat"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "Enter custom model name (used when model_select is 'Custom')",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "deepseek-chat"  # é»˜è®¤æ¨¡å‹
        return DeepSeekConnectorGeneral(api_token, model),


class SetGeminiLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "gemini-1.5-pro",
                        "gemini-1.5-flash",
                        "gemini-2.5-pro",
                        "gemini-2.5-flash",
                        "Custom",
                    ],
                    {"default": "gemini-2.5-pro"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "Enter custom model name (used when model_select is 'Custom')",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "gemini-2.5-pro"
        return GeminiConnectorGeneral(api_token, model),


class SetBailianLLMServiceConnector(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_token": ("STRING", {"default": ""}),
                "model_select": (
                    [
                        "qwen-plus",
                        "qwen-max",
                        "qwen-flash",
                        "qwen-turbo",
                        "qwen-long",
                        "Custom",
                    ],
                    {"default": "qwen-flash"},
                ),
            },
            "optional": {
                "custom_model": (
                    "STRING",
                    {
                        "default": "",
                        "placeholder": "è‡ªå®šä¹‰æ¨¡å‹åï¼ˆå½“é€‰æ‹©Customæ—¶ç”Ÿæ•ˆï¼‰",
                    },
                ),
            },
        }

    RETURN_TYPES = ("LLMServiceConnector",)
    RETURN_NAMES = ("llm_service_connector",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, api_token, model_select, custom_model=""):
        model = model_select if model_select != "Custom" else custom_model
        if not model:
            model = "qwen-flash"
        return BailianLLMServiceConnector(api_token, model),


class CheckLLMServiceConnectivity(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_service_connector": ("LLMServiceConnector",),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("log",)
    FUNCTION = "execute"
    CATEGORY = MY_CATEGORY

    def execute(self, llm_service_connector):
        try:
            # åªå‘ä¸€ä¸ªç©ºæ¶ˆæ¯ï¼ˆæœ‰äº›APIéœ€è¦messagesè‡³å°‘æœ‰ä¸€æ¡ï¼Œç»™ä¸ªç®€å•çš„æç¤ºï¼‰
            test_messages = [{"role": "user", "content": "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ"}]
            result = llm_service_connector.invoke(test_messages)
            # åªè¦æ²¡æŠ¥é”™ï¼Œè¯´æ˜æœåŠ¡å¯è”é€š
            return mie_log(f"LLMæœåŠ¡æ¥å£å¯è”é€š (HTTP 200 + æ­£å¸¸å“åº”), è¿”å›å†…å®¹: {result}"),
        except Exception as e:
            return mie_log(f"LLMæœåŠ¡æ£€æµ‹å¤±è´¥: {str(e)}"),


# é€šç”¨è°ƒç”¨èŠ‚ç‚¹ï¼šå¯¹æ¥ä»»æ„å·²åˆ›å»ºçš„ LLMServiceConnector
class CallLLMService(object):
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_service_connector": ("LLMServiceConnector",),
                "input_text": ("STRING", {"default": "", "multiline": True}),
            },
            "optional": {
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0}),
                "max_tokens": ("INT", {"default": 512, "min": 1}),
                "seed": ("INT", {"default": 0, "min": 0}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response",)
    FUNCTION = "call"
    CATEGORY = MY_CATEGORY

    def call(self, llm_service_connector, input_text, temperature=0.7, top_p=0.9, max_tokens=512, seed=None):
        """
        ä¸€ä¸ªç®€å•çš„é€šç”¨èŠ‚ç‚¹ï¼Œå°†çº¯æ–‡æœ¬åŒ…è£…ä¸ºç”¨æˆ·æ¶ˆæ¯å¹¶è°ƒç”¨ä»»æ„ LLMServiceConnector çš„ invoke æ–¹æ³•ã€‚
        è¯¥èŠ‚ç‚¹ä¸ä¼šæ”¹å˜åº•å±‚ connector çš„è¡Œä¸ºæˆ– stateã€‚
        """
        messages = [{"role": "user", "content": input_text}]
        # å°†å¯é€‰å‚æ•°ç›´æ¥è½¬å‘ç»™ connector.invoke
        result = llm_service_connector.invoke(messages, seed=seed, temperature=temperature, top_p=top_p,
                                              max_tokens=max_tokens)
        return (result.strip(),)
